#!/usr/bin/env python3
"""
ERROR MODEL - The Convergence Engine
Implements the Kalman Filter-based error minimization system
This is the core innovation that makes LEX-7 deterministic instead of probabilistic
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Dict, List, Any
import logging
from pathlib import Path
import json
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ConvergenceMethod(Enum):
    """Methods for error convergence"""
    KALMAN_FILTER = "kalman_filter"
    PID = "pid"
    ADAPTIVE = "adaptive"
    GRADIENT_DESCENT = "gradient_descent"

@dataclass
class ErrorState:
    """Represents the current error state of the system"""
    error_magnitude: float
    error_vector: torch.Tensor
    convergence_rate: float
    stability_indicator: float
    divergence_risk: float

@dataclass
class ControlSignal:
    """Represents the control signal generated by the error model"""
    correction_magnitude: float
    correction_direction: torch.Tensor
    confidence: float
    convergence_action: str

class KalmanFilter:
    """
    Kalman Filter implementation for error state estimation
    
    This is the mathematical foundation for the Error-State Model.
    Instead of predicting "what comes next," it calculates "how to correct error."
    """
    
    def __init__(self, state_dim: int, observation_dim: int, dt: float = 1.0):
        self.state_dim = state_dim
        self.observation_dim = observation_dim
        self.dt = dt
        
        # State transition matrix (A)
        self.A = torch.eye(state_dim, dtype=torch.float32)
        self.A[0, 1] = dt  # Position-velocity relationship
        
        # Input transition matrix (B)
        self.B = torch.zeros(state_dim, observation_dim, dtype=torch.float32)
        self.B[0, 0] = 0.5 * dt * dt  # Position control
        self.B[1, 1] = dt  # Velocity control
        
        # Observation matrix (H)
        self.H = torch.zeros(observation_dim, state_dim, dtype=torch.float32)
        self.H[0, 0] = 1.0  # Can observe position
        self.H[1, 1] = 1.0  # Can observe velocity
        
        # Process noise covariance (Q)
        self.Q = torch.eye(state_dim, dtype=torch.float32) * 0.01
        
        # Observation noise covariance (R)
        self.R = torch.eye(observation_dim, dtype=torch.float32) * 0.1
        
        # State estimate and covariance
        self.x_est = torch.zeros(state_dim, dtype=torch.float32)
        self.P = torch.eye(state_dim, dtype=torch.float32)
        
        logger.info(f"Initialized Kalman Filter: {state_dim}D state, {observation_dim}D observations")
    
    def predict(self, u: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict the next state
        
        Args:
            u: Control input [observation_dim]
            
        Returns:
            x_pred: Predicted state
            P_pred: Predicted covariance
        """
        # State prediction: x_pred = A * x_est + B * u
        if u is None:
            u = torch.zeros(self.observation_dim, dtype=torch.float32)
        
        x_pred = torch.matmul(self.A, self.x_est) + torch.matmul(self.B, u)
        
        # Covariance prediction: P_pred = A * P * A^T + Q
        P_pred = torch.matmul(torch.matmul(self.A, self.P), self.A.T) + self.Q
        
        return x_pred, P_pred
    
    def update(
        self, 
        x_pred: torch.Tensor, 
        P_pred: torch.Tensor, 
        z: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Update the state estimate with new observation
        
        Args:
            x_pred: Predicted state
            P_pred: Predicted covariance
            z: Observation [observation_dim]
            
        Returns:
            x_est: Updated state estimate
            P_est: Updated covariance
            K: Kalman gain
        """
        # Innovation: y = z - H * x_pred
        y = z - torch.matmul(self.H, x_pred)
        
        # Innovation covariance: S = H * P_pred * H^T + R
        S = torch.matmul(torch.matmul(self.H, P_pred), self.H.T) + self.R
        
        # Kalman gain: K = P_pred * H^T * S^(-1)
        K = torch.matmul(torch.matmul(P_pred, self.H.T), torch.inverse(S))
        
        # State update: x_est = x_pred + K * y
        x_est = x_pred + torch.matmul(K, y)
        
        # Covariance update: P_est = (I - K * H) * P_pred
        I = torch.eye(self.state_dim, dtype=torch.float32)
        P_est = torch.matmul(I - torch.matmul(K, self.H), P_pred)
        
        return x_est, P_est, K
    
    def step(
        self, 
        u: torch.Tensor, 
        z: torch.Tensor
    ) -> Tuple[ErrorState, ControlSignal]:
        """
        Single Kalman filter step
        
        Args:
            u: Control input
            z: Observation
            
        Returns:
            error_state: Current error state
            control_signal: Control signal for convergence
        """
        # Predict
        x_pred, P_pred = self.predict(u)
        
        # Update
        x_est, P_est, K = self.update(x_pred, P_pred, z)
        
        # Store estimates
        self.x_est = x_est
        self.P = P_est
        
        # Compute error metrics
        error_vector = x_est[:self.observation_dim] - z
        error_magnitude = torch.norm(error_vector).item()
        
        # Compute convergence rate
        P_trace = torch.trace(P_est).item()
        convergence_rate = 1.0 / (1.0 + P_trace)
        
        # Compute stability indicator
        stability = torch.det(P_est).item()
        
        # Compute divergence risk
        divergence_risk = max(0.0, error_magnitude - convergence_rate)
        
        # Generate control signal
        control_gain = 1.0  # Could be adaptive
        correction_magnitude = error_magnitude * control_gain
        
        if error_magnitude > 0:
            correction_direction = -error_vector / error_magnitude
        else:
            correction_direction = torch.zeros_like(error_vector)
        
        confidence = min(1.0, convergence_rate)
        
        # Determine convergence action
        if error_magnitude < 0.01:
            action = "converged"
        elif error_magnitude < 0.1:
            action = "fine_tuning"
        elif divergence_risk > 0.5:
            action = "emergency_correction"
        else:
            action = "normal_correction"
        
        error_state = ErrorState(
            error_magnitude=error_magnitude,
            error_vector=error_vector,
            convergence_rate=convergence_rate,
            stability_indicator=stability,
            divergence_risk=divergence_risk
        )
        
        control_signal = ControlSignal(
            correction_magnitude=correction_magnitude,
            correction_direction=correction_direction,
            confidence=confidence,
            convergence_action=action
        )
        
        return error_state, control_signal

class PIDController:
    """
    PID Controller for error correction
    
    Simpler alternative to Kalman Filter for less complex scenarios
    """
    
    def __init__(self, kp: float = 1.0, ki: float = 0.1, kd: float = 0.05):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        
        self.integral = 0.0
        self.prev_error = 0.0
        
        logger.info(f"Initialized PID Controller: Kp={kp}, Ki={ki}, Kd={kd}")
    
    def step(self, error: float, dt: float = 1.0) -> ControlSignal:
        """
        Single PID step
        
        Args:
            error: Current error value
            dt: Time step
            
        Returns:
            control_signal: Control signal for error correction
        """
        # Proportional term
        P = self.kp * error
        
        # Integral term
        self.integral += error * dt
        I = self.ki * self.integral
        
        # Derivative term
        derivative = (error - self.prev_error) / dt
        D = self.kd * derivative
        
        # Total control signal
        control_output = P + I + D
        
        # Update for next iteration
        self.prev_error = error
        
        # Create control signal
        correction_magnitude = abs(control_output)
        
        if error != 0:
            correction_direction = torch.tensor([-np.sign(control_output)], dtype=torch.float32)
        else:
            correction_direction = torch.tensor([0.0], dtype=torch.float32)
        
        confidence = min(1.0, 1.0 / (1.0 + abs(error)))
        
        if abs(error) < 0.01:
            action = "converged"
        elif abs(error) < 0.1:
            action = "fine_tuning"
        else:
            action = "normal_correction"
        
        return ControlSignal(
            correction_magnitude=correction_magnitude,
            correction_direction=correction_direction,
            confidence=confidence,
            convergence_action=action
        )

class AdaptiveErrorModel:
    """
    Adaptive Error Model that switches between methods based on error characteristics
    
    This is the intelligent coordinator that decides which convergence method to use
    """
    
    def __init__(self, state_dim: int, method: ConvergenceMethod = ConvergenceMethod.KALMAN_FILTER):
        self.state_dim = state_dim
        self.method = method
        
        # Initialize convergence methods
        self.kalman_filter = KalmanFilter(state_dim, min(state_dim, 2))
        self.pid_controller = PIDController()
        
        # Performance tracking
        self.performance_history = []
        self.method_switches = 0
        
        # Adaptation parameters
        self.adaptation_window = 100
        self.switch_threshold = 0.1
        
        logger.info(f"Initialized Adaptive Error Model using {method.value}")
    
    def analyze_error_characteristics(self, error_history: List[float]) -> Dict[str, float]:
        """Analyze error characteristics to determine best method"""
        if len(error_history) < 10:
            return {"noise_level": 0.0, "drift_rate": 0.0, "complexity": 0.0}
        
        recent_errors = error_history[-10:]
        
        # Noise level (variance)
        noise_level = np.var(recent_errors)
        
        # Drift rate (trend)
        drift_rate = np.polyfit(range(len(recent_errors)), recent_errors, 1)[0]
        
        # Complexity (non-linearity)
        complexity = np.std(np.diff(recent_errors))
        
        return {
            "noise_level": noise_level,
            "drift_rate": abs(drift_rate),
            "complexity": complexity
        }
    
    def select_optimal_method(self, error_chars: Dict[str, float]) -> ConvergenceMethod:
        """Select the optimal convergence method based on error characteristics"""
        
        # Decision tree for method selection
        if error_chars["noise_level"] < 0.01:
            # Low noise environment
            if error_chars["drift_rate"] < 0.001:
                return ConvergenceMethod.KALMAN_FILTER  # Stable environment
            else:
                return ConvergenceMethod.PID  # Some drift
        elif error_chars["complexity"] > 0.1:
            # High complexity, use adaptive
            return ConvergenceMethod.ADAPTIVE
        else:
            # Standard case
            return ConvergenceMethod.KALMAN_FILTER
    
    def step(
        self, 
        current_state: torch.Tensor, 
        target_state: torch.Tensor, 
        control_input: Optional[torch.Tensor] = None
    ) -> Tuple[ErrorState, ControlSignal]:
        """
        Main error model step with adaptive method selection
        
        Args:
            current_state: Current system state
            target_state: Target/desired state
            control_input: External control input
            
        Returns:
            error_state: Current error state
            control_signal: Correction signal
        """
        # Compute current error
        error_vector = target_state - current_state
        error_magnitude = torch.norm(error_vector).item()
        
        # Update performance history
        self.performance_history.append(error_magnitude)
        if len(self.performance_history) > self.adaptation_window:
            self.performance_history.pop(0)
        
        # Select optimal method
        if len(self.performance_history) >= 10:
            error_chars = self.analyze_error_characteristics(self.performance_history)
            optimal_method = self.select_optimal_method(error_chars)
            
            if optimal_method != self.method:
                self.method = optimal_method
                self.method_switches += 1
                logger.info(f"Switched convergence method to {optimal_method.value}")
        
        # Apply selected method
        if self.method == ConvergenceMethod.KALMAN_FILTER:
            observation = current_state[:self.kalman_filter.observation_dim]
            if control_input is None:
                control_input = torch.zeros(self.kalman_filter.observation_dim)
            
            error_state, control_signal = self.kalman_filter.step(control_input, observation)
            
        elif self.method == ConvergenceMethod.PID:
            error_state = ErrorState(
                error_magnitude=error_magnitude,
                error_vector=error_vector,
                convergence_rate=0.0,  # Will be computed
                stability_indicator=0.0,
                divergence_risk=0.0
            )
            control_signal = self.pid_controller.step(error_magnitude)
            error_state.convergence_rate = control_signal.confidence
            error_state.stability_indicator = control_signal.confidence
            error_state.divergence_risk = max(0.0, error_magnitude - control_signal.confidence)
            
        else:  # ADAPTIVE or other
            # Fallback to Kalman Filter
            observation = current_state[:min(len(current_state), 2)]
            if control_input is None:
                control_input = torch.zeros_like(observation)
            
            error_state, control_signal = self.kalman_filter.step(control_input, observation)
        
        return error_state, control_signal
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """Get performance metrics for monitoring"""
        if not self.performance_history:
            return {"avg_error": 0.0, "min_error": 0.0, "max_error": 0.0, "stability": 0.0}
        
        errors = self.performance_history
        return {
            "avg_error": np.mean(errors),
            "min_error": np.min(errors),
            "max_error": np.max(errors),
            "stability": 1.0 - np.std(errors),
            "method_switches": self.method_switches,
            "current_method": self.method.value
        }

class SovereignDirectiveValidator:
    """
    Validates and enforces Sovereign Directives
    
    This ensures that the Lex Node stays aligned with the user's axioms and directives
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.sovereign_state = None
        self.directive_history = []
        self.compliance_scores = []
        
        # Load sovereign axioms (in production, this would load from files)
        self.sovereign_axioms = self.load_sovereign_axioms()
        
        logger.info("Initialized Sovereign Directive Validator")
    
    def load_sovereign_axioms(self) -> List[Dict]:
        """Load sovereign axioms from configuration"""
        # In production, this would load from JSON files
        return [
            {
                "id": "financial_discipline",
                "type": "wealth_preservation",
                "rule": "never_spend_more_than_available_runway",
                "weight": 1.0,
                "mandatory": True
            },
            {
                "id": "time_efficiency",
                "type": "productivity",
                "rule": "optimize_all_activities_for_maximum_output",
                "weight": 0.8,
                "mandatory": False
            },
            {
                "id": "health_priority",
                "type": "vitality",
                "rule": "never_compromise_long_term_health_for_short_term_gains",
                "weight": 0.9,
                "mandatory": True
            }
        ]
    
    def validate_directive(
        self, 
        directive: Dict[str, Any], 
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Validate a BARK directive against sovereign axioms
        
        Args:
            directive: The BARK directive to validate
            context: Additional context for validation
            
        Returns:
            validation_result: Validation result with compliance score
        """
        # Extract directive content
        command = directive.get("command", "")
        parameters = directive.get("parameters", {})
        signature = directive.get("signature", "")
        timestamp = directive.get("timestamp", "")
        
        # Check signature validity (simplified)
        if not self.verify_signature(directive):
            return {
                "valid": False,
                "reason": "invalid_signature",
                "compliance_score": 0.0,
                "required_corrections": []
            }
        
        # Check compliance with sovereign axioms
        compliance_results = []
        required_corrections = []
        
        for axiom in self.sovereign_axioms:
            compliance = self.check_axiom_compliance(command, parameters, axiom)
            compliance_results.append(compliance)
            
            if compliance["score"] < self.config.get("compliance_threshold", 0.95):
                if axiom["mandatory"]:
                    return {
                        "valid": False,
                        "reason": f"violates_mandatory_axiom_{axiom['id']}",
                        "compliance_score": compliance["score"],
                        "required_corrections": [axiom["rule"]]
                    }
                else:
                    required_corrections.append(compliance["suggestion"])
        
        # Compute overall compliance score
        total_score = sum(r["score"] * r["weight"] for r in compliance_results)
        total_weight = sum(r["weight"] for r in compliance_results)
        overall_score = total_score / total_weight if total_weight > 0 else 1.0
        
        # Record validation
        self.compliance_scores.append(overall_score)
        self.directive_history.append({
            "directive": directive,
            "compliance_score": overall_score,
            "timestamp": timestamp
        })
        
        return {
            "valid": True,
            "compliance_score": overall_score,
            "required_corrections": required_corrections,
            "axiom_compliance": compliance_results
        }
    
    def verify_signature(self, directive: Dict[str, Any]) -> bool:
        """
        Verify the cryptographic signature of a directive
        
        In production, this would implement proper cryptographic verification
        """
        # Simplified verification - check required fields
        required_fields = ["command", "signature", "timestamp"]
        return all(field in directive for field in required_fields)
    
    def check_axiom_compliance(
        self, 
        command: str, 
        parameters: Dict, 
        axiom: Dict
    ) -> Dict[str, Any]:
        """
        Check compliance with a specific sovereign axiom
        
        Args:
            command: The command from the directive
            parameters: The parameters from the directive
            axiom: The sovereign axiom to check against
            
        Returns:
            compliance_result: Result of compliance check
        """
        axiom_type = axiom["type"]
        rule = axiom["rule"]
        weight = axiom["weight"]
        
        # Simplified compliance checking logic
        if axiom_type == "wealth_preservation":
            # Check if directive violates financial discipline
            if "spend" in command.lower() or "purchase" in command.lower():
                if "budget" in parameters:
                    budget = parameters.get("budget", 0)
                    available_runway = parameters.get("available_runway", float('inf'))
                    
                    if budget > available_runway * 0.1:  # Spending more than 10% of runway
                        return {
                            "score": 0.0,
                            "weight": weight,
                            "suggestion": "Reduce spending to maintain runway",
                            "axiom_id": axiom["id"]
                        }
        
        elif axiom_type == "health_priority":
            # Check if directive compromises health
            if "skip" in command.lower() and "sleep" in command.lower():
                return {
                    "score": 0.2,
                    "weight": weight,
                    "suggestion": "Maintain adequate sleep schedule",
                    "axiom_id": axiom["id"]
                }
        
        elif axiom_type == "time_efficiency":
            # Check if directive optimizes time usage
            if "task" in parameters:
                estimated_time = parameters.get("estimated_time", 0)
                expected_output = parameters.get("expected_output", 0)
                
                if estimated_time > 0 and expected_output > 0:
                    efficiency = expected_output / estimated_time
                    if efficiency < 0.5:  # Low efficiency threshold
                        return {
                            "score": 0.3,
                            "weight": weight,
                            "suggestion": "Optimize task for better efficiency",
                            "axiom_id": axiom["id"]
                        }
        
        # Default: assume compliance
        return {
            "score": 1.0,
            "weight": weight,
            "suggestion": None,
            "axiom_id": axiom["id"]
        }
    
    def update_sovereign_state(self, validation_results: List[Dict]):
        """Update the persistent sovereign state"""
        if not validation_results:
            return
        
        # Compute weighted average compliance
        total_score = sum(r["compliance_score"] for r in validation_results)
        avg_compliance = total_score / len(validation_results)
        
        if self.sovereign_state is None:
            self.sovereign_state = avg_compliance
        else:
            # Exponential moving average
            alpha = 0.1
            self.sovereign_state = alpha * avg_compliance + (1 - alpha) * self.sovereign_state
    
    def get_compliance_statistics(self) -> Dict[str, float]:
        """Get statistics about compliance performance"""
        if not self.compliance_scores:
            return {"avg_compliance": 1.0, "min_compliance": 1.0, "max_compliance": 1.0}
        
        return {
            "avg_compliance": np.mean(self.compliance_scores),
            "min_compliance": np.min(self.compliance_scores),
            "max_compliance": np.max(self.compliance_scores),
            "compliance_variance": np.var(self.compliance_scores),
            "total_directives_validated": len(self.compliance_scores)
        }

# Example usage and testing
if __name__ == "__main__":
    # Test the Error Model
    error_model = AdaptiveErrorModel(state_dim=4096)
    
    # Simulate error convergence
    current_state = torch.randn(4096)
    target_state = torch.zeros(4096)
    
    for i in range(10):
        error_state, control_signal = error_model.step(current_state, target_state)
        print(f"Step {i+1}: Error={error_state.error_magnitude:.4f}, Action={control_signal.convergence_action}")
        
        # Simulate state correction
        current_state = current_state + control_signal.correction_direction * control_signal.correction_magnitude
    
    # Test Sovereign Directive Validator
    config = {"compliance_threshold": 0.95}
    validator = SovereignDirectiveValidator(config)
    
    sample_directive = {
        "command": "analyze_spending",
        "parameters": {"amount": 500, "category": "entertainment"},
        "signature": "test_signature",
        "timestamp": "2025-11-28T08:51:00Z"
    }
    
    result = validator.validate_directive(sample_directive)
    print(f"Validation result: {result}")
